[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Welcome to my blog",
    "section": "",
    "text": "Here you’ll find some info on my blog etc.\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nData Analytics Capstone\n\n\n\nR\n\n\n\n\n\n\n\nSimon R. Steinkamp\n\n\nMay 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Covid-19 Dashboard (for Denmark) using Dash and Plotly\n\n\n\nPython\n\n\nPlotly\n\n\nDash\n\n\n\n\n\n\n\nSimon R. Steinkamp\n\n\nMar 19, 2021\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n30th place solution in Kaggle\n\n\n\nPython\n\n\nKaggle\n\n\n\n\n\n\n\nSimon R. Steinkamp\n\n\nJul 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReaction Time Analysis: The exponential Gaussian\n\n\n\nReaction Times\n\n\nModeling\n\n\nR\n\n\n\nReaction time analysis, using an Exgaus.\n\n\n\nSimon R. Steinkamp\n\n\nApr 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHIDA-Datathon in April\n\n\n\nPython\n\n\nHIDA\n\n\n\n\n\n\n\nSimon R. Steinkamp\n\n\nApr 30, 2020\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nA Pandas surprise - NaNs with groupby\n\n\n\nPython\n\n\n\n\n\n\n\nSimon R. Steinkamp\n\n\nApr 28, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Since August 2021 I am a Postdoc in the Computational Neuroscience of Reward group at the Danish Research Center for Magnetic Resonance (DRCMR), where we are investigating whether and how the principles of Ergodicity Economics (it’s fascinating, check it out: https://ergodicityeconomics.com/) apply to human decision-making.\nThe project is not only behavioral, as we are also investigating, whether these principles can be found in the reinforcement error signals in the human reward system (e.g., in ensembles of dopamine neurons). In doing this, we developed a new method for mapping parameters of computational models onto single voxels (see the poster).\nMy doctorate research was on the computational modeling of visual attention, most of it using neuroimaging (fMRI) in combination with dynamic causal modeling (DCM). During my masters I also used electrophysiological measures (EEG) for speech recovery, in combination with fMRI recordings.\nDuring my master’s degree I also started developing and coding in Python, Matlab, and R and started out on Kaggle, as well as different scientific hackathons on neuroimaging and climate change."
  },
  {
    "objectID": "posters.html",
    "href": "posters.html",
    "title": "Posters",
    "section": "",
    "text": "In this poster, we present our first examples of Computational Parametric Mapping."
  },
  {
    "objectID": "posters/2022-08-25-CCN.html",
    "href": "posters/2022-08-25-CCN.html",
    "title": "Conference on Cognitive Computational Neuroscience 2022",
    "section": "",
    "text": "See here our poster presented at CCN 2022 during Poster Session 2, the manuscript can be found here.\n\n\n\nThis browser does not support PDFs. Please download the PDF to view it: Download PDF."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simon R. Steinkamp",
    "section": "",
    "text": "Construction warning: I previously used a mixture of academic pages and nbpages for my blog and the other content - currently migrating to a single Quarto page - a few things might break or change."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Simon R. Steinkamp",
    "section": "Welcome",
    "text": "Welcome\nYou are now on my landing page. From here you can go further to my blog / collection and other of my projects. In the navbar on top you can find my attempts at blogging, publications, CV (soon), and other things I produced in the last years."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Simon R. Steinkamp",
    "section": "Projects",
    "text": "Projects\nHere are a few projects of mine I want to highlight, please see their respective GitHub pages.\n\npymtrf\nMy first “full-blown” project. For a presentation I decided to translate the MTRF-toolbox for Matlab to Python and unleash the fullstack of continuous integration and testing suits on it.\nLink to the Github Repro\n\n\nMaskbinarizer\nCurrently I am using nbdev for developing, especially due to the easy creation of documentations. This is a very basic command line tool, to binarize NIFTI files: The last time I checked mricron outputs brain masks in RGB, while a popular lesion symptom mapping tool expects binary images.\nLink to Docs\nLink to Repro"
  },
  {
    "objectID": "blog/2020-04-28-Pandas-GroupBy-NaN.html",
    "href": "blog/2020-04-28-Pandas-GroupBy-NaN.html",
    "title": "A Pandas surprise - NaNs with groupby",
    "section": "",
    "text": "A Pandas surprise - NaNs and groupby\n\nimport pandas as pd\nimport numpy as np\nfrom pandas.errors import UnsupportedFunctionCall\n\nI figured out something about pandas today, which I was very surprised by. Applying .groupby on a pd.DataFrame automatically ignores NaN values. This is intended behavior, but sometimes you actually want to have some NaN in the data, to check whether your data-frame is correct and to find possible corruptions.\nHere is a little example:\n\n# Create a sample Array:\nDF = pd.DataFrame.from_dict({'g1': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], \n                             'g2': ['c', 'c', 'd', 'd', 'c', 'c', 'd', 'd'],\n                             'd1': [0, 1, np.nan, 3, 4, 5, 6, 7]})\n\nAveraging the entries in DF, we would expect a NaN in group a, d, but we get 3.0!\n\nDF.groupby(['g1', 'g2']).mean()\n\nIf you apply pandas .mean() method on a DataFrame you could specify a skipna = False in the function. This, unfortunately doesn’t work after using .groupby.\n\n# This creates an error\ntry:\n    DF.groupby(['g1', 'g2']).mean(skipna=False)\nexcept UnsupportedFunctionCall:\n    print('UnsupportedFunctionCall')\n\nI think, I have seen one solution to solve this issue stating that using .apply(np.mean) instead of using .mean() might solve the problem.\nHowever:\n\nDF.groupby(['g1', 'g2']).apply(np.mean)\n\nCalling np.mean causes pandas to bypass the function and calls DF.mean() from pandas with skipna=True! As far as I know, you have to create a new function to solve the issue.\n\ndef mean_w_nan(x):\n    # Don't forget the np.array call!\n    return np.mean(np.array(x))\n\nDF.groupby(['g1', 'g2']).apply(mean_w_nan)\n\n\n\nReferences:\n\nhttps://stackoverflow.com/questions/26145585/pandas-aggregation-ignoring-nans\nhttps://github.com/pandas-dev/pandas/issues/15674\nhttps://stackoverflow.com/questions/54106112/pandas-groupby-mean-not-ignoring-nans"
  },
  {
    "objectID": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html",
    "href": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html",
    "title": "Reaction Time Analysis: The exponential Gaussian",
    "section": "",
    "text": "This is the non-interactive Version of the analysis! You can find the interactive notebook here, caution it’s about 7MB in size, so not your average website!\nAll code can be found in rmarkdown documents on Github https://github.com/SRSteinkamp/ReactionTimeWrangling/exgauss\nAnalyzing reaction times has a long history in psychology and (cognitive) neuroscience. Most people who studied Psychology in university have conducted classical psychological experiments. For example investigating the Stroop task. You might have encountered the stimuli, with the typical task to spell out loud as fast as possible written color names:\n\n\\(\\\\color{red}{\\\\text{RED}}\\)\n\\(\\\\color{yellow}{\\\\text{BLUE}}\\)\n\\(\\\\color{blue}{\\\\text{YELLOW}}\\)\n\\(\\\\color{green}{\\\\text{GREEN}}\\)\n\nAfter running such a task, with many more word-color pairs and different participants, a typical question in undergrad might be: Are participants significantly faster while reading matching color words (\\(\\\\color{green}{\\\\text{GREEN}}\\)) than reading non-matching color words (\\(\\\\color{blue}{\\\\text{GREEN}}\\))? A straightforward answer would be to average the reaction times in the non-matching and matching conditions for each participant and then compare the two conditions using for example a paired t-test. Yes, if p < 0.05 and no, if p > 0.05.\nResults are in and everything is fine? If you start wondering whether this is the correct way of analysis, you might find more and more and is more. Different discussions about:\n\nShould conditions across participants be averaged using the mean or the median?\nShould data be averaged at all?\nHow to define outliers?\n…\nCould a drift-diffusion model provide the key insights?\n…\nIs null hypothesis significance testing meaningful?\n\nAll of these questions are not really in my main field of expertise (no worries I won’t deal with the last one ;) , but I found reading about reaction time analyses weirdly entertaining and very interesting, and I wanted to start with blogging. So here is the first one of a couple of experiments I am planning to do. There is no particular order but all are based on some questions that arose while looking at different papers.\nSo maybe there is something useful here for you, or not. Or you disagree or have comments, suggestions, etc. please get in touch!\nHere is the first part:"
  },
  {
    "objectID": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#part-1---how-many-trials-do-i-need-to-fit-an-ex-gauss",
    "href": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#part-1---how-many-trials-do-i-need-to-fit-an-ex-gauss",
    "title": "Reaction Time Analysis: The exponential Gaussian",
    "section": "Part 1 - How many trials do I need to fit an Ex-Gauss?",
    "text": "Part 1 - How many trials do I need to fit an Ex-Gauss?\nData below is generated sampling 100000 observations from two Ex-Gaussian distributions with μ = 300, σ = 20, τ = 300 (red) and μ = 500, σ = 50, τ = 100. Note, that both distributions have the same mean of 600 (the black dashed line). To the left the two components of the Ex-Gaussian are presented - the Gaussian and the Exponential distributions. Here again the dashed-lines describe the mean of the corresponding distributions. \nThe Ex(ponential) Gauss(ian) distribution, is the sum of a Gaussian distribution parametrized by μ, and σ, which define the “body” of the distribution, with an Exponential function (τ) describing the skew to the right. The normally distributed body, with a long tail, has been found to closely match the distribution of reaction time data found in many experiments (Palmer et al. 2011). While the fit to experimental data seems to be ideal, the parameters itself do not seem to be related to any specific cognitive constructs. At least, the discussion is still ongoing (Spieler, Balota, and Faust 2000). The strength of fitting distributions to reaction times is seen in the ability to provide a finer description than for example a summary of a certain condition using the mean or the median. Different combinations of the Ex-Gauss parameters μ and τ, for example can lead to the same mean. So comparing two conditions for example might provide the same summary statistics, but the distributions might have a very different spread and skew.\nIf you are interested in checking out more distributions (and more about model fitting, etc.) visit this great page: https://lindeloev.github.io/shiny-rt/\nThis is just a quick introduction into why fitting a distribution might provide a better picture of reaction times, but how many trials are necessary per condition?"
  },
  {
    "objectID": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#methods",
    "href": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#methods",
    "title": "Reaction Time Analysis: The exponential Gaussian",
    "section": "Methods",
    "text": "Methods\nTo simulate data I used the 12 Ex-Gauss distributions used by Miller (1988), and many others. My assumption is, that a researcher wants to fit an Ex-Gauss function for each condition and each participant in an experiment. Note that, there are ways to estimate distributions across multiple participants, which seem to be stable, even for small numbers of trials(Ratcliff 1979), which I am not (yet?) going into. One estimate is that around 100 trials might be needed to get reliable results (Ratcliff 1979).\nHere I want to investigate how many trials are needed, and whether there are general biases in the estimation. For most of the analysis I am using the retimes package. Data is simulated using rexgauss and for model fitting using both the method of moments (mexgauss) and maximum likelihood estimation (MLE) timefit are used. According to the documentation timefit gets its starting parameters using the method of moments.\nI am simulating data from the 12 distributions starting with 10 (maybe a rare-condition, like an oddball), up to 500 trials (a Psychophysicist’s dream (Palmer et al. 2011)). For each of the twelve distributions I sampled different numbers of trials (10, 20, 35, 50, 100, 200, 350, 500) and then estimated the three parameters μ, σ, τ using the method of moments and MLE. This process was repeated 10000 times.\n\n\n\nDistribution\nMu\nSigma\nTau\n\n\n\n\n1\n300\n20\n300\n\n\n2\n300\n50\n300\n\n\n3\n350\n20\n250\n\n\n4\n350\n50\n250\n\n\n5\n400\n20\n200\n\n\n6\n400\n50\n200\n\n\n7\n450\n20\n150\n\n\n8\n450\n50\n150\n\n\n9\n500\n20\n100\n\n\n10\n500\n50\n100\n\n\n11\n550\n20\n50\n\n\n12\n550\n50\n50"
  },
  {
    "objectID": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#results",
    "href": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#results",
    "title": "Reaction Time Analysis: The exponential Gaussian",
    "section": "Results",
    "text": "Results\nThe histograms below describe our simulation results. Feel free to click around (in the interactive version) and select different distributions and methods. From our visual inspection we can see that using a small number of trials can lead to quite some biases in the estimated parameters. The spread of estimated parameters decreases the more samples are considered. The data in the histograms is filtered to only include parameter estimates greater than 0 and less than 750. The histograms are calculated so that 40 bins for each sample are estimated and only for sample sizes of 20, 100, 500). This might not be the best way to display the data, but was done to keep the size of the HTML as small as possible. Furthermore, some data cleaning had to be performed as there are raw-events with very unlikely parameter estimates. In the non-interactive version, there estimates for distribution 6 only, however you can see Moments and MLE side by side.\n\nInteractive Version Legend\nI couldn’t figure out how to put meaningful legends on the interactive version: * red estimates for μ, solid bars distribution μ * blue estimates for τ, dotted bars distribution τ * green estimates for σ, dashed bars distribution σ  To get an estimate of how well (or bad) the modeling performed, I calculated the mean error to investigate general trends, its standard-deviation (SD), and the mean absolute error for error estimation (MAE).\nThere is unfortunately, too much data to have good look at, so here is a datatable to play around with and investigate some of the summary values (in the interactive version only). Data can be created using the .Rmd files in the Repro"
  },
  {
    "objectID": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#analysis",
    "href": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#analysis",
    "title": "Reaction Time Analysis: The exponential Gaussian",
    "section": "Analysis",
    "text": "Analysis\nPlotting the summaries for the estimations and pooling over distributions, we can draw first (maybe obvious) conclusions:\n\nlarger sample sizes, lead to less error\nmaximum likelihood estimation performs generally better, than the method of moments.\n\nInterestingly, regardless of estimation procedure, the σ and μ parameters seem to be overestimated, whereas τ is underestimated.\n\n\nStatistical Summary\n\nHow strong are the observed biases?\n As we have already seen in the figure, we confirm that μ is generally overestimated while, τ is underestimated. This makes sense given the distribution of the data: The majority of data will be sampled from the Gaussian part of the distribution, so extreme-values are relatively rare. It is therefore much harder to correctly estimate the skew (τ). As the mean of the Ex-Gaussian is given by μ + τ, a underestimation of τ automatically leads to a larger estimate of μ.\nAlso, the method of moments seems to be more prone to biases, especially leading to an overestimation of σ.\n\n\nWhat can we learn about the error?\n\nAll factors which were included in the model appear to have some meaning (are significant). As we have seen in the other analysis, the method of moments has a higher base MAE rate than the maximum likelihood estimation. Furthermore, we can see that μ and τ are easier identifiable the farther they are apart, especially when the method of moments is used. This is expressed by the regressor diff_mt = μ − τ (based on the original distributions). And again: larger sample sizes are the key factor to reduce the error!"
  },
  {
    "objectID": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#conclusion",
    "href": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#conclusion",
    "title": "Reaction Time Analysis: The exponential Gaussian",
    "section": "Conclusion",
    "text": "Conclusion\nIf you want to get a good estimate of the reaction time distribution: collect enough data!\nTo provide a bit more nuance, you can get away with small sample sizes, if you are lucky. For example if the reaction time distribution isn’t very skewed. But keep in mind, that you can expect a higher error in the estimation of the true parameters (especially using the method of moments), when τ and μ are close to each other. Looking at the histograms, we also see that there is a lot of variance in the estimation of σ. So further analyzing σ, for example in a group analysis, should be done very carefully. Last but not least, sample sizes should be equal when comparing μ and τ across different conditions! Even if parameters are drawn from the same Ex-Gauss distribution, it is very likely that the condition with fewer trials will have a higher estimate of these parameters. We can do a small simulation of this using our simulated data.\n\nType 1 error due to imbalanced sample sizes\nFor simplicity, I decided to only use samples from distribution 6 (with μ = 400, τ = 200, and σ = 50), estimated by MLE. I am drawing 30 random sets of estimated parameters for different combinations of sample sizes. The number 30 is quite arbitrary but is supposed to reflect a typical number of participants in a reaction time experiment. The parameters of the different distributions are then submitted to a paired two-sided t-test and the number of significant results (p < 0.05) are reported. In theory, as data is drawn from the same distribution, we should expect around 5% false positive results.\nHistogram of the simulation results. We can see that the first two pairs have many false positives (p < 0.05, left of the black line). The later pairings on the other hand seem to have a rather uniform distribution of p-values (as we would expect). The pairings (e.g., 20 : 50) show on how many trials the parameters in condition 1 (20) and in condition 2 (50) were estimated. The paired t-tests were then calculated as condition 1 > condition 2. \nFor completeness’ sake, I also calculated the average t-value for each of the pairing, next to the proportion of false positive results.\n\n\n\nPairing\nMu_p\nMu_t\nTau_p\nTau_t\n\n\n\n\n1 - 20 : 50\n0.248\n1.406\n0.166\n-1.065\n\n\n2 - 20 : 200\n0.378\n1.762\n0.256\n-1.407\n\n\n3 - 50 : 100\n0.053\n0.289\n0.057\n-0.293\n\n\n4 - 50 : 200\n0.060\n0.434\n0.068\n-0.408\n\n\n5 - 100 : 200\n0.049\n0.142\n0.056\n-0.092\n\n\n6 - 200 : 200\n0.050\n-0.009\n0.051\n0.005\n\n\n\nAs assumed, we have an inflation of false-positive t-tests when comparing estimates of Ex-Gauss parameters from the same distribution (but estimated using different sample-sizes). The larger the imbalance, the larger the false positive rate!\n\n\nConclusions not related to analysis\nThis is the first larger project I conducted in R(markdown). I really don’t like the basic R syntax in many regards, but using dplyr and the rest of the tidyverse is great :) %>% it all the way! Building interactive figures with ggplot2 + plotly + crosstalk is also quite amazing. But as I did not want to create a shiny app, figuring out how to deal with exploding sizes of .html files took me quite some time."
  },
  {
    "objectID": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#references",
    "href": "blog/2020-05-01-Reaction-Time-Analysis-Ex-Gauss/index.html#references",
    "title": "Reaction Time Analysis: The exponential Gaussian",
    "section": "References",
    "text": "References\nMiller, Jeff. 1988. “A Warning About Median Reaction Time.” Journal of Experimental Psychology: Human Perception and Performance 14 (3): 539–43. https://doi.org/10.1037/0096-1523.14.3.539.\nPalmer, Evan M., Todd S. Horowitz, Antonio Torralba, and Jeremy M. Wolfe. 2011. “What Are the Shapes of Response Time Distributions in Visual Search?” Journal of Experimental Psychology: Human Perception and Performance 37 (1): 58–71. https://doi.org/10.1037/a0020747.\nRatcliff, Roger. 1979. “Group Reaction Time Distributions and an Analysis of Distribution Statistics.” Psychological Bulletin 86 (3): 446–61. https://doi.org/10.1037/0033-2909.86.3.446.\nSpieler, Daniel H., David A. Balota, and Mark E. Faust. 2000. “Levels of Selective Attention Revealed Through Analyses of Response Time Distributions.” Journal of Experimental Psychology: Human Perception and Performance 26 (2): 506–26. https://doi.org/10.1037/0096-1523.26.2.506."
  },
  {
    "objectID": "blog/2020-04-03-HIDA-datathon/index.html",
    "href": "blog/2020-04-03-HIDA-datathon/index.html",
    "title": "HIDA-Datathon in April",
    "section": "",
    "text": "02-03 April 2020 HIDA-datathon\n\nThis remote/online 36h datathon was organized by the Helmholtz Information & Data Science Academy, inplace of the big datathon: Grand Challenges on Climate Change, which couldn’t take place due to the coronavirus.\nThis was a very interesting experience, participating in a challenge to analyze and find structure in previously unknown dataformats (especially map information) and fields (geo/climate science). After the challenge was stated, we realized that there were a couple of teams who had really concrete ideas as how to analyze the data and what kind of computational models to use.\nOur team agnostic decided that not assuming anything in particular would be the best approach, and using our very different background and levels of expertise, we were able to put together a good summary of the data at hand. Deciding to throw many conventions out of the window, we also found out some things that were surprising to the domain experts, for example that longitudinal data (even in summary chunks) provides valuable information for modeling aspects."
  },
  {
    "objectID": "blog/2021-05-21-Analytics-Capstone/index.html",
    "href": "blog/2021-05-21-Analytics-Capstone/index.html",
    "title": "Data Analytics Capstone",
    "section": "",
    "text": "Somewhere between defending my doctorate and looking for a job, I found myself doing the Google Analytics Capstone on Coursera which provided nice insights into data analytics and the thinking behind it. But unfortunately for me the technical level was very basic (beginner friendly) so that I did not learn many things I had hoped for.\nAnyways, here is the URL to the capstone project, where I selected the provided idea of creating a report for bike-sharing data.\nReport\nThe whole repro can be found here."
  },
  {
    "objectID": "blog/2020-07-03-The-30th-Place-Solution.html",
    "href": "blog/2020-07-03-The-30th-Place-Solution.html",
    "title": "30th place solution in Kaggle",
    "section": "",
    "text": "This post is also available in the Kaggle forums.\nFirst of all thanks to the organizers to putting together this challenging and interesting competition!\nI’ve seen many good and well written and well developed solutions on the challenge … mine is neither. But I just wanted chip in, as I might have used some feature engineering that hasn’t been used as widely. Unfortunately I don’t have the time right now to dig into it in detail, but will just give you a rough description about what I did.\n\n\nA personal note on how I perceived the challenge. I was really exciting to see this challenge, this should lie in my area of expertise, so why not give it a try. So I did, wrote a for my terms successful EDA notebook, got into the data and somehow got to 13th place. But, as people noted, there was not much going on in terms of discussion etc., so I might have gotten a bit complacent with my position and did not develop much further. Also I might have gotten a bit annoyed, because I couldn’t get my errors on domain1_var1 and the domain2 vars done as much as I liked.\nThen life happened, vacation, deadlines, an online hackathon. Not much space for the challenge, and suddenly there are 10 days left, I have a deadline coming up, and find myself around place 50th on the leaderboard. So some panic sets in and also some craziness, so I rewrote the analysis, and preprocessing from scratch, run all the different models. Stack them up and well suddenly it’s place 34, and 30 on the private leaderboard - my best competition so far.\n\n\n\n\n\n\nI decided to perform stratified cross-validation on my data, so I used KNN(n_clusters=15) on the target variables, setting the missing values to 0, so that this will be taken into account for the stratification approach.\nThen I imputed the missing data using sklearn.impute.IterativeImputer to get values there. This seemed to be a good balance for me between discarding the data or just filling in a constant. However, I didn’t really look into the imputations afterwards…\n\n\n\n\n\nloading.csv - Discarded IC_20 - Seemed to be very different between train and test (discussions and kernels)\nfnc.csv - Reordering the data for use with nilearn.connectome.vec_to_sym_matrix\nExtracting data from the 4D feature maps. I have a kernel out-there showing how this can be done using different Masker objects using nilearn.input_data. I extracted the sets schaeffer (400 brain regions), msdl, and basc (197 regions) here.\nGraph-based features: Several publications also focus on using graph-theory based descriptions of functional connectivity as features for machine learning classifiers. So I used the “Brain Connectivity Toolbox”, bct.py to derive features. These were just a ton of global and local descriptions, also at different thresholdings of the connectivity matrices (derived from fnc). Around 1700 features in total (with sometimes really bad numerical issues), which I somehow hacked away in the preprocessing before my stacking data. This will be named fnc_graph.\nCombined data, I also created the set loadfnc, combining fnc and loading data, and loadmsdl combining msdl and loading. Finally, I also used a loadnoise set, where I added some random intercept and Gaussian noise to the loading data (differently for each subset). If that helped at all, I couldn’t unfortunately test.\n\n\n\n\n\nI stacked tons of models (32 for each feature) using different regression approaches, and sometimes different preprocessing. ConnectivityMeasure (shortened to CM). is a class from nilearn.connectome, that can be used to transform a n x t matrix in to a n x n connectivity matrix, using differnt kinds of connectivity. The nice thing is, it also fits into sklearn-pipelines as a vectorized version of the matrix is possible.\n\n\n\n\n\n\n\n\nData Set\nPreprocessing SVR, LGBM\nPreprocessing Regression\n\n\n\n\nbasc\nCM(tangent), PCA(whiten=True), RobustScaler()\nCM(correlation), PCA(whiten=True)\n\n\nmsdl\nCM(tangent), PCA(whiten=True), RobustScaler()\nCM(correlation), PCA(whiten=True)\n\n\nschaeffer\nCM(tangent), PCA(whiten=True), RobustScaler()\nCM(correlation), PCA(whiten=True)\n\n\nfnc\nPCA(whiten=True)\nabs(fnc) < 0.15 = 0, PCA(whiten=True)\n\n\nfnc_pca\nNone\nabs(fnc) < 0.15 = 0, None\n\n\nloading\nRobustScaler()\nRobustScaler()\n\n\nfnc_graph\nNumerical fixes, PCA(whiten=True), RobustScaler()\nNumerical fixes, PCA(whiten=True), RobustScaler()\n\n\nloadmsdl\nPCA(whiten=True), RobustScaler()\nPCA(whiten=True), RobustScaler()\n\n\nloadfnc\nRobustScaler()\nRoubstScaler()\n\n\nloadnoise\nRobustScaler()\nRobustScaler()\n\n\n\n\n\nI used sklearns SVR using both a linear and a rbf kernel on the datasets basc, msdl, schaeffer, fnc_pca, loading, and fnc_graph. To figure out the best parameters I applied skopt.BayesianSearchCV with 35 iterations (objective mean absolute error). Parameters optimized where C, epsilon and n_components for PCAs.\nSo here are 2 x 6 = 12 models.\n\n\n\nIn this competition I somehow came to like the LassoLars regression of sklearn. So that’s what I am using here. The feature selection of it seemed to help actually. Running models on basc, msdl, schaeffer, fnc, loading, fnc_graph, loadmsdl, loadfnc, and loadnoise. This time optimizing mean squared error using BayesianSearchCV for alpha and n_components.\nSo 9 models.\n\n\n\nSame datasets as for Regression. Optimizing tree parameters and PCA, best model defined by mean absolute error.\nAnother 9 models.\n\n\n\nI also tried to get some more spatial information into the model as well so I set up a small 2D CNN having:\n\nConv2D Layer, with ReLU activation\nMaxpooling (2, 2)\nFlatten\nDropout()\nDense(1)\n\nWhere the number of filters, kernel_size, Dropout, learning_rate, and the loss (mae, mse), where found through BayesianSearchCV.\n\n\n\n\nI used the same approach for all models and the final stacking model:\n\nOptimize hyperparameters on 5-Folds.\nRetrain model on CV-Data\nEvaluate on hold-out set\nRetrain on all data\nPredict on test set.\n\nThe final stacking model was again a LassoLars regression, on the outputs of the 31 models. I actually preprocessed the predictions, by slapping a RobustScaler in just for good measure.\n\n\n\nI learned quite a lot from the competition, but have to say that I am not really satisfied with what I did (my best competition so far…), and see a lot of room for improvement.\n\n\nI think the most annoying part for me is, that I just stacked tons of models. In the end not even thinking much about why I am doing it. I just wanted to get that 0.001 Leaderboard boost, to get a little edge. But, if I had invested my time more into careful tuning, preprocessing, and careful model selection, I think I would have gotten more out in the last weekend of the competition, than I did here. In the end, I was mostly waiting for models to finish running and to start the next set of long calculations.\n\n\n\nSo far, my intuition on evaluating locally and avoiding overfitting got me quite far (I got my first silver medal basically because of an incredibly heavy shake up of the leaderboard, pushing me a couple of 100 places or so to the top). Here I think I was actually quite lucky - see the forum posts where people discuss about the lack of a shake up.\n\n\n\nNext time I am in this situation, I think I will team up. Also I apologize to the people who contacted me and I didn’t get back to. Here it was mostly bad timing, but I think there is some much to gain in terms of insights, when you can discuss your solutions :)\nAnd of course much more."
  },
  {
    "objectID": "blog/2021-03-19-Covid19-Dashboard/index.html",
    "href": "blog/2021-03-19-Covid19-Dashboard/index.html",
    "title": "A Covid-19 Dashboard (for Denmark) using Dash and Plotly",
    "section": "",
    "text": "The website presented here is currently not working - statsbank changed the API and I think my heroku has expired…"
  },
  {
    "objectID": "blog/2021-03-19-Covid19-Dashboard/index.html#tldr",
    "href": "blog/2021-03-19-Covid19-Dashboard/index.html#tldr",
    "title": "A Covid-19 Dashboard (for Denmark) using Dash and Plotly",
    "section": "TL;DR",
    "text": "TL;DR\nI created a small dashboard for COVID-19 cases in Denmark, deployed on heroku.\nWhich you can see here"
  },
  {
    "objectID": "blog/2021-03-19-Covid19-Dashboard/index.html#background",
    "href": "blog/2021-03-19-Covid19-Dashboard/index.html#background",
    "title": "A Covid-19 Dashboard (for Denmark) using Dash and Plotly",
    "section": "Background",
    "text": "Background\nTo motivate myself to learn a few new skills which might come handy in future jobs, I got a few courses on udemy as a general guidance and more structured approach to get into new topics.\nOne of the courses was on designing interactive dashboards in Pythons using plotly and dash. As a code-along capstone they showed how to develop a dashboard for the stock-market. Now, I could not easily get the data I wanted through pandas-datareader, as many of the APIs now require a registration or are paid."
  },
  {
    "objectID": "blog/2021-03-19-Covid19-Dashboard/index.html#the-data",
    "href": "blog/2021-03-19-Covid19-Dashboard/index.html#the-data",
    "title": "A Covid-19 Dashboard (for Denmark) using Dash and Plotly",
    "section": "The Data",
    "text": "The Data\nAs I was anyways up for a bit more of a challenge and recently moved to Denmark and was digging regularly digging through COVID-19 Dashboards (like the one at zeit.de), I gave myself a try.\nFortunately, Denmark provides regularly updated data on COVID-19 (and many many other topics) at statbank.dk all accessible via a POST API.\nWhile the data was easy accessible and could be directly read into pandas as a .csv, it actually had internally some interesting challenges. For example, querying dates that were not present in the database led to errors, so that I had to retrieve meta-data about the database before creating queries, so that I could filter data beforehand."
  },
  {
    "objectID": "blog/2021-03-19-Covid19-Dashboard/index.html#the-dashboard",
    "href": "blog/2021-03-19-Covid19-Dashboard/index.html#the-dashboard",
    "title": "A Covid-19 Dashboard (for Denmark) using Dash and Plotly",
    "section": "The Dashboard",
    "text": "The Dashboard\nThe Dashboard itself is a basic line-plot (or a collection of line-plots). To keep the traffic as low as possible, I decided, to leave the update button from the code-along capstone next to the date-range picker and the selection of communes in Denmark. In fact, clicking update requests the data and downloads it into the html file.\nI here decided to store the data as an invisible json in the page itself, which was provided as a possible solution in the dash-documentation.\nThis ways the updates to the plot using the radio-buttons does not need to download new data and can do the computations, without constantly querying the database.\n\n\n\nDashboard overview.\n\n\nTo see the dashboards source code please visit: https://github.com/SRSteinkamp/DK_Covid19_Dash"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Steinkamp, S. R., Fink, G. R., Vossel, S., & Weidner, R. 2021. Simultaneous Modeling of Reaction Times and Brain Dynamics in a Spatial Cueing Task, Human Brain Mapping 25758; https:\\doi.org/10.1002/hbm.25758\nSteinkamp, S. R., Vossel, S., Fink, G. R., & Weidner, R. 2020. Attentional reorientation along the meridians of the visual field: Are there different neural mechanisms at play? Human Brain Mapping, 25086. https://doi.org/10.1002/hbm.25086\nBotvinik-Nezer, R., Holzmeister, F., Camerer, C. F., Dreber, A., Huber, J., Johannesson, M., … Steinkamp, S. R. … Schonberg, T. 2020. Variability in the analysis of a single neuroimaging dataset by many teams. Nature. https://doi.org/10.1038/s41586-020-2314-9\nPuschmann, S., Steinkamp, S., Gillich, I., Mirkovic, B., Debener, S., & Thiel, C. M. 2017. The Right Temporoparietal Junction Supports Speech Tracking During Selective Listening: Evidence from Concurrent EEG-fMRI. The Journal of Neuroscience, 37(47), 11505. https://doi.org/10.1523/JNEUROSCI.1007-17.2017"
  }
]